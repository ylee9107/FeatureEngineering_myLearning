{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Feature Construction\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "This project will primarily focus on working with categorical data to build new features into the dataset so that the models employed will be able to learn from them. The idea is to introduce and construct features that are useful to improve the model's performance over the baseline.\n",
    "\n",
    "\n",
    "## Breakdown of this Project:\n",
    "- Examining the Dataset\n",
    "- Applying imputation techniques to categorical features.\n",
    "- Applying encoding techniques to categorical variables.\n",
    "- Extending numerical features.\n",
    "- Text-specific feature construction.\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Taking a look at the Dataset:\n",
    "\n",
    "The dataset here will be self-created to show a variety of data leels and types.\n",
    "\n",
    "In this section, the Pandas Dataframe with its attributes and methods will be used on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Required Libraries:\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Dataset:\n",
    "X_data = pd.DataFrame({'city':['tokyo', None, 'london', 'seattle', 'san francisco', 'tokyo'],\n",
    "                       'boolean':['yes', 'no', None, 'no', 'no', 'yes'], \n",
    "                       'ordinal_column':['somewhat like', 'like', 'somewhat like', 'like', \n",
    "                                         'somewhat like', 'dislike'], \n",
    "                       'quantitative_column':[1, 11, -.5, 10, None, 20]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>boolean</th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>london</td>\n",
       "      <td>None</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seattle</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>san francisco</td>\n",
       "      <td>no</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city boolean ordinal_column  quantitative_column\n",
       "0          tokyo     yes  somewhat like                  1.0\n",
       "1           None      no           like                 11.0\n",
       "2         london    None  somewhat like                 -0.5\n",
       "3        seattle      no           like                 10.0\n",
       "4  san francisco      no  somewhat like                  NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, the table consists of several columns that are:\n",
    "- boolen, is a binary categorical data (e.g. yes or no) and this is at the nominal level.\n",
    "- city, is categorical data and is at the nominal level.\n",
    "- ordinal_column, is a column of ordinal data and at an ordinal level.\n",
    "- quantitative_column, contains integers that are at the ratio level.\n",
    "\n",
    "## 2 - Peform Imputation on the Categorical Features:\n",
    "\n",
    "With the understanding of the dataset outlined above, this section will go through the imputation process.\n",
    "\n",
    "### 2.1 - Dealing with Missing Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city                   1\n",
       "boolean                1\n",
       "ordinal_column         0\n",
       "quantitative_column    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find missing values in the dataset:\n",
    "X_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that there are 3 missing values in the stated columns. These value will require imputation.\n",
    "\n",
    "Next, a custom transformer will be implemented, where it is a method that will impute the missing values in a column.\n",
    "\n",
    "#### For the \"City\" Column:\n",
    "\n",
    "As mentioned, this column is categorical, which means the imputation strategy is fill the missing with the most common category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokyo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most common category in this column:\n",
    "X_data['city'].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Tokyo was the most frequent. Next is to impute the missing row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            tokyo\n",
       "1            tokyo\n",
       "2           london\n",
       "3          seattle\n",
       "4    san francisco\n",
       "5            tokyo\n",
       "Name: city, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data['city'].fillna(value=X_data['city'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the imputation worked nicely, the next part would be to fix the remaining categorical columns. To do this, a custom imputer will be made.\n",
    "\n",
    "### 2.2 -  Build Custom Imputer:\n",
    "\n",
    "Pipeline are an assembly of steps (transformations) that can be cross-validated together while allowing the setting of different parameters.\n",
    "\n",
    "Building a pipeline allows for the following:\n",
    "1. Enables the application of sequential list of transformations before a final estimator.\n",
    "2. Each intermediate steps of the pipeline are \"transforms\" (a fit and transform method).\n",
    "3. The last layer is the final Estimator (which is a fit method).\n",
    "\n",
    "The pipeline will have the built transformers for each of the coloumns that requires imputing, where the dataset will be passed and transformed in one go. \n",
    "\n",
    "#### Custom Category Imputer:\n",
    "\n",
    "Here, the \"TransformerMixin\" class from scikit-learn will be utlised to build the custom categorical imputer. Note that this transformer is only one of the element in the pipeline, where in this case, it would be dealing with categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required library:\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Custom category imputer Class:\n",
    "\n",
    "class CustomCategoryImputer(TransformerMixin):\n",
    "    \"\"\" This builds the Custom Category Imputer, that inherits the TransformerMixin class.\n",
    "        The inheritance should have a .fit_transform method to call with .fit and .transform methods.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise one instance attribute, the columns:\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "    \n",
    "    # Fill the missing column values:\n",
    "    def transform(self, dataFrame):\n",
    "        X = dataFrame.copy()\n",
    "        \n",
    "        for col in self.cols:\n",
    "            X[col].fillna(value=X[col].value_counts().index[0], inplace=True)\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    # Fit method, that follows the fit method from scikit-learn:\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above imputer completed, it can be used on the \"city\" and \"boolean\" (categorical) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>boolean</th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>london</td>\n",
       "      <td>no</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seattle</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>san francisco</td>\n",
       "      <td>no</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>dislike</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city boolean ordinal_column  quantitative_column\n",
       "0          tokyo     yes  somewhat like                  1.0\n",
       "1          tokyo      no           like                 11.0\n",
       "2         london      no  somewhat like                 -0.5\n",
       "3        seattle      no           like                 10.0\n",
       "4  san francisco      no  somewhat like                  NaN\n",
       "5          tokyo     yes        dislike                 20.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the custom imputer, instantiate:\n",
    "cci = CustomCategoryImputer(cols=['city', 'boolean'])\n",
    "\n",
    "# Fit and transform on the dataset:\n",
    "cci.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values in both columns have now been filled.\n",
    "\n",
    "### 2.3 - Build Custom Quantitative Imputer:\n",
    "\n",
    "The Custom Quantitative Imputer will be similar to the previous one, but will contain an added \"strategy\" parameter that accounts for quantitative data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required package:\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Custom Quantitative imputer Class:\n",
    "\n",
    "class CustomQuantitativeImputer(TransformerMixin):\n",
    "    \"\"\" This builds the Custom Quantitative Imputer, that inherits the TransformerMixin class.\n",
    "        The inheritance should have a .fit_transform method to call with .fit and .transform methods.\n",
    "    Note:\n",
    "        - requires SimpleImputer from sklearn.impute .\n",
    "    \"\"\"\n",
    "    # Initialise instance attributes, the columns and a strategy:\n",
    "    def __init__(self, cols=None, strategy='mean'):\n",
    "        self.cols = cols\n",
    "        self.strategy = strategy\n",
    "    \n",
    "    # Fill the missing column values:\n",
    "    def transform(self, dataFrame):\n",
    "        X = dataFrame.copy()\n",
    "        \n",
    "        imputer = SimpleImputer(strategy=self.strategy)\n",
    "        \n",
    "        # Note the double brackets:\n",
    "        for col in self.cols:\n",
    "            X[col] = imputer.fit_transform(X[[col]])\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    # Fit method, that follows the fit method from scikit-learn:\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above imputer completed, it can be used on the \"quantitative_column\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>boolean</th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>london</td>\n",
       "      <td>None</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seattle</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>san francisco</td>\n",
       "      <td>no</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>dislike</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city boolean ordinal_column  quantitative_column\n",
       "0          tokyo     yes  somewhat like                  1.0\n",
       "1           None      no           like                 11.0\n",
       "2         london    None  somewhat like                 -0.5\n",
       "3        seattle      no           like                 10.0\n",
       "4  san francisco      no  somewhat like                  8.3\n",
       "5          tokyo     yes        dislike                 20.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the custom imputer, instantiate:\n",
    "cqi = CustomQuantitativeImputer(cols=['quantitative_column'], strategy='mean')\n",
    "\n",
    "# Fit and transform on the dataset:\n",
    "cqi.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Implement Imputers in the Pipeline:\n",
    "\n",
    "Use both the CustomCategoryImputer and CustomQuantitativeImputer in a pipeline to impute the missing values for both Categorical and Quantitative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required library:\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>boolean</th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>london</td>\n",
       "      <td>no</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seattle</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>san francisco</td>\n",
       "      <td>no</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>dislike</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city boolean ordinal_column  quantitative_column\n",
       "0          tokyo     yes  somewhat like                  1.0\n",
       "1          tokyo      no           like                 11.0\n",
       "2         london      no  somewhat like                 -0.5\n",
       "3        seattle      no           like                 10.0\n",
       "4  san francisco      no  somewhat like                  8.3\n",
       "5          tokyo     yes        dislike                 20.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the custom imputer, instantiate:\n",
    "cci = CustomCategoryImputer(cols=['city', 'boolean'])\n",
    "\n",
    "# Apply the custom imputer, instantiate:\n",
    "cqi = CustomQuantitativeImputer(cols=['quantitative_column'], strategy='mean')\n",
    "\n",
    "# Pipeline:\n",
    "pipeline_imputer = Pipeline(steps=[('quant', cqi), \n",
    "                                   ('category', cci)] \n",
    "                           )\n",
    "\n",
    "# Fit and transform on the dataset:\n",
    "pipeline_imputer.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Applying encoding techniques to categorical variables:\n",
    "\n",
    "For the compatibility with ML models, categorical values/data will have to be __converted to numerical data__. This is because, ML models all require numerical values to work with.\n",
    "\n",
    "### 3.1 - Encoding Data at the Nominal Level:\n",
    "\n",
    "To do this, there are two options to transform the categorical data into dummy variables:\n",
    "1. Pandas libray is able to find categorical values and dummy code them.\n",
    "2. Create a custom transformer using dummy variables to work in a pipeline.\n",
    "\n",
    "__Dummy Variables__: These are 0 or 1 values that indicates the presence of a category. In other words these are proxy variables for qualitative data.\n",
    "\n",
    "__Dummy Variable Trap__: This is where independent variables are multicollinear or highly correlated, meaning that these variables can be predicted from each other. __To avoid this trap__, is to leave out the constant term or one of the dummy categories. The left out dummy variable can then become the base to be compared with for the rest of the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 - Pandas libray is able to find categorical values and dummy code them:\n",
    "\n",
    "Note that the Ordinal columns should not be dummified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "      <th>city_london</th>\n",
       "      <th>city_san francisco</th>\n",
       "      <th>city_seattle</th>\n",
       "      <th>city_tokyo</th>\n",
       "      <th>boolean_no</th>\n",
       "      <th>boolean_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>somewhat like</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>somewhat like</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>somewhat like</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dislike</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ordinal_column  quantitative_column  city_london  city_san francisco  \\\n",
       "0  somewhat like                  1.0            0                   0   \n",
       "1           like                 11.0            0                   0   \n",
       "2  somewhat like                 -0.5            1                   0   \n",
       "3           like                 10.0            0                   0   \n",
       "4  somewhat like                  NaN            0                   1   \n",
       "5        dislike                 20.0            0                   0   \n",
       "\n",
       "   city_seattle  city_tokyo  boolean_no  boolean_yes  \n",
       "0             0           1           0            1  \n",
       "1             0           0           1            0  \n",
       "2             0           0           0            0  \n",
       "3             1           0           1            0  \n",
       "4             0           0           1            0  \n",
       "5             0           1           0            1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the columns to dummify:\n",
    "pd.get_dummies(data=X_data,\n",
    "               prefix_sep='_',\n",
    "               columns= ['city', 'boolean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 - Create a custom transformer using dummy variables to work in a pipeline:\n",
    "\n",
    "Using a custom transformer allows the set up of the pipeline to transform the whole dataset in one go. Note, this mimics the scikit-learn's one-hot encoding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDummifier(TransformerMixin):\n",
    "    \"\"\" This builds the Custom Dummifier, that inherits the TransformerMixin class.\n",
    "        The inheritance should have a .fit_transform method to call with .fit and .transform methods.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise one instance attribute, the columns\n",
    "    def __init__(self, cols=None):\n",
    "        self.cols = cols\n",
    "    \n",
    "    # Transform the dataset with the dummy variables:\n",
    "    def transform(self, X):\n",
    "        return pd.get_dummies(data=X,\n",
    "                              prefix_sep='_',\n",
    "                              columns= self.cols)\n",
    "    \n",
    "    # Fit method, that follows the fit method from scikit-learn:\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "      <th>boolean_no</th>\n",
       "      <th>boolean_yes</th>\n",
       "      <th>city_london</th>\n",
       "      <th>city_san francisco</th>\n",
       "      <th>city_seattle</th>\n",
       "      <th>city_tokyo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>somewhat like</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>somewhat like</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>somewhat like</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dislike</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ordinal_column  quantitative_column  boolean_no  boolean_yes  city_london  \\\n",
       "0  somewhat like                  1.0           0            1            0   \n",
       "1           like                 11.0           1            0            0   \n",
       "2  somewhat like                 -0.5           0            0            1   \n",
       "3           like                 10.0           1            0            0   \n",
       "4  somewhat like                  NaN           1            0            0   \n",
       "5        dislike                 20.0           0            1            0   \n",
       "\n",
       "   city_san francisco  city_seattle  city_tokyo  \n",
       "0                   0             0           1  \n",
       "1                   0             0           0  \n",
       "2                   0             0           0  \n",
       "3                   0             1           0  \n",
       "4                   1             0           0  \n",
       "5                   0             0           1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the custom Dummifier, instantiate:\n",
    "cd = CustomDummifier(cols= ['boolean', 'city'])\n",
    "\n",
    "# Fit and transform on the dataset:\n",
    "cd.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it can be seen that the dummy values were applied successfully by the customDummifier class. The next column to consider is the Oridinal Column.\n",
    "\n",
    "### 3.2 - Encoding Data at the Ordinal Level:\n",
    "\n",
    "Here, the Oridinal data are in strings, and these will also be needed to be converted to numerical data. At the Ordinal Level, the data have a specific order, therefore the __Label Encoder__ is used to ensure the order is maintained, rather than just using the dummy variables.\n",
    "\n",
    "The data is processed in such a way that each of the label in the ordinal data will have a numerical value that is associated to it, meaning \"labeling\" each of the ordinal values uniquely with a numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "5    2\n",
      "Name: ordinal_column, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Find the ordering of the Ordinal Values:\n",
    "ordering = list( X_data['ordinal_column'].unique() )\n",
    "\n",
    "print(X_data['ordinal_column'].map(lambda x: ordering.index(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 - Create a custom Label Encoder for Ordinal Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEncoder(TransformerMixin):\n",
    "    \"\"\" This builds the Custom Label Encoder, that inherits the TransformerMixin class.\n",
    "        The inheritance should have a .fit_transform method to call with .fit and .transform methods.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise the instance attributes, the columns and ordering:\n",
    "    def __init__(self, col=None, ordering=None):\n",
    "        self.col = col\n",
    "        self.ordering = ordering\n",
    "    \n",
    "    # Transform the dataset's ordinal values:\n",
    "    def transform(self, dataFrame):\n",
    "        X = dataFrame.copy()\n",
    "        \n",
    "        X[self.col] = X[self.col].map(lambda x: ordering.index(x))\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    # Fit method, that follows the fit method from scikit-learn:\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above class implemented, apply it on the ordinal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>boolean</th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>london</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seattle</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>san francisco</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city boolean  ordinal_column  quantitative_column\n",
       "0          tokyo     yes               0                  1.0\n",
       "1           None      no               1                 11.0\n",
       "2         london    None               0                 -0.5\n",
       "3        seattle      no               1                 10.0\n",
       "4  san francisco      no               0                  NaN\n",
       "5          tokyo     yes               2                 20.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the ordering of the Ordinal Values:\n",
    "ordering = list( X_data['ordinal_column'].unique() )\n",
    "\n",
    "# Apply the custom imputer, instantiate:\n",
    "ce = CustomEncoder(col='ordinal_column', ordering=ordering)\n",
    "\n",
    "# Fit and transform on the dataset:\n",
    "ce.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the oridnal values are re-labeled with numerical values.\n",
    "\n",
    "### 3.3 - Bucketing the Continuous Features into Categories:\n",
    "\n",
    "Another way of treating continuous numerical features is to bucket them into ranges, transforming a continuous variable into categorical variable.(Examples, age ranges, salary ranges etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     (-0.52, 6.333]\n",
       "1    (6.333, 13.167]\n",
       "2     (-0.52, 6.333]\n",
       "3    (6.333, 13.167]\n",
       "4                NaN\n",
       "5     (13.167, 20.0]\n",
       "Name: quantitative_column, dtype: category\n",
       "Categories (3, interval[float64]): [(-0.52, 6.333] < (6.333, 13.167] < (13.167, 20.0]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bining (create a range) the \"quatitative_column\":\n",
    "pd.cut(x=X_data['quantitative_column'], bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    0.0\n",
       "3    1.0\n",
       "4    NaN\n",
       "5    2.0\n",
       "Name: quantitative_column, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bining (create a range) the \"quatitative_column\", with labels set to false:\n",
    "pd.cut(x=X_data['quantitative_column'], bins=3, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 - Create a Custom Cutter for the Continuous Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCutter(TransformerMixin):\n",
    "    \"\"\" This builds the Custom Cutter, that inherits the TransformerMixin class.\n",
    "        The inheritance should have a .fit_transform method to call with .fit and .transform methods.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialise the instance attributes, the columns, bins and labels:\n",
    "    def __init__(self, col, bins, labels=False):\n",
    "        self.col = col\n",
    "        self.bins = bins\n",
    "        self.labels = labels\n",
    "    \n",
    "    # Transform the dataset's Continuous values:\n",
    "    def transform(self, dataFrame):\n",
    "        X = dataFrame.copy()\n",
    "        \n",
    "        X[self.col] = pd.cut(x=X[self.col], bins=self.bins, labels=self.labels)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    # Fit method, that follows the fit method from scikit-learn:\n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>boolean</th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>london</td>\n",
       "      <td>None</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seattle</td>\n",
       "      <td>no</td>\n",
       "      <td>like</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>san francisco</td>\n",
       "      <td>no</td>\n",
       "      <td>somewhat like</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>yes</td>\n",
       "      <td>dislike</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city boolean ordinal_column  quantitative_column\n",
       "0          tokyo     yes  somewhat like                  0.0\n",
       "1           None      no           like                  1.0\n",
       "2         london    None  somewhat like                  0.0\n",
       "3        seattle      no           like                  1.0\n",
       "4  san francisco      no  somewhat like                  NaN\n",
       "5          tokyo     yes        dislike                  2.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the custom Cutter, instantiate:\n",
    "cc = CustomCutter(col='quantitative_column', bins=3, labels=False)\n",
    "\n",
    "# Fit and transform on the dataset:\n",
    "cc.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the Continuous Variables in the \"quantitative_column\" have been imputed into its respective bins. \\\n",
    "Categories (3, interval[float64]): [(-0.52, 6.333] < (6.333, 13.167] < (13.167, 20.0]]\n",
    "\n",
    "## 4 - Create the Pipeline:\n",
    "\n",
    "This section will transform the dataset accordingly with the above mentioned imputation classes, where it will convert the values into ML model ready numerical values. It will do this will a Pipeline Implementation.\n",
    "\n",
    "The sequence of the Pipeline:\n",
    "1. Impute the Missing Values\n",
    "2. Dummify the categorical columns.\n",
    "3. Encode the Ordinal column.\n",
    "3. Bucketise the Quatitative column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "\n",
    "# Apply the custom imputer, instantiate:\n",
    "cci = CustomCategoryImputer(cols=['city', 'boolean'])\n",
    "\n",
    "# Apply the custom imputer, instantiate:\n",
    "cqi = CustomQuantitativeImputer(cols=['quantitative_column'], strategy='mean')\n",
    "\n",
    "# Define the imputer: for quantitative and categorical values.\n",
    "imputer = Pipeline(steps=[('quant', cqi),\n",
    "                          ('category', cci)]\n",
    "                  )\n",
    "# ==============================================================================\n",
    "\n",
    "# Apply the custom dummifier, instantiate:\n",
    "cd = CustomDummifier(cols= ['boolean', 'city'])\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "# Find the ordering of the Ordinal Values:\n",
    "ordering = list( X_data['ordinal_column'].unique() )\n",
    "\n",
    "# Apply the custom imputer, instantiate:\n",
    "ce = CustomEncoder(col='ordinal_column', ordering=ordering)\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "# Apply the custom Cutter, instantiate:\n",
    "cc = CustomCutter(col='quantitative_column', bins=3, labels=False)\n",
    "\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ordinal_column</th>\n",
       "      <th>quantitative_column</th>\n",
       "      <th>boolean_no</th>\n",
       "      <th>boolean_yes</th>\n",
       "      <th>city_london</th>\n",
       "      <th>city_san francisco</th>\n",
       "      <th>city_seattle</th>\n",
       "      <th>city_tokyo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ordinal_column  quantitative_column  boolean_no  boolean_yes  city_london  \\\n",
       "0               0                    0           0            1            0   \n",
       "1               1                    1           1            0            0   \n",
       "2               0                    0           1            0            1   \n",
       "3               1                    1           1            0            0   \n",
       "4               0                    1           1            0            0   \n",
       "5               2                    2           0            1            0   \n",
       "\n",
       "   city_san francisco  city_seattle  city_tokyo  \n",
       "0                   0             0           1  \n",
       "1                   0             0           1  \n",
       "2                   0             0           0  \n",
       "3                   0             1           0  \n",
       "4                   1             0           0  \n",
       "5                   0             0           1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Pipeline Sequence:\n",
    "pipe = Pipeline(steps=[(\"imputer\", imputer), \n",
    "                       (\"dummify\", cd), \n",
    "                       (\"encode\", ce), \n",
    "                       (\"cut\", cc)]\n",
    "               )\n",
    "\n",
    "# Fit the Pipeline to the Dataset:\n",
    "pipe.fit(X_data)\n",
    "\n",
    "# Transform the Dataset:\n",
    "pipe.transform(X_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, all the values in the dataset are now in Numerical form, which is compatible with ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Extending numerical features:\n",
    "\n",
    "For numerical features, it is also to create extended features by undergoing through vaarious methods. Here a new dataset will be utilised to demonstrate the process. \n",
    "\n",
    "### 5.1 - Dataset: Activity recognition from a Single Chest Mounted Accelerometer dataset.\n",
    "\n",
    "Link: https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer\n",
    "\n",
    "Dataset Description: It is a collection of data from a wearable accelerometer that is mounted on the chest of 15 subjects. These subjects performs 7 activities and the accelerometer records the activity data. The sampling frequency of the accelerometer is set to 52Hz and is presented to be uncalibrated.\n",
    "\n",
    "Attribute Information:\n",
    "- Data are separated by participant\n",
    "- Each file contains the following information:\n",
    "    1. sequential number, \n",
    "    2. x acceleration, \n",
    "    3. y acceleration, \n",
    "    4. z acceleration, \n",
    "    5. label\n",
    "\n",
    "Labels are codified by numbers:\n",
    "1. Working at Computer\n",
    "2. Standing Up, Walking and Going updown stairs\n",
    "3. Standing\n",
    "4. Walking\n",
    "5. Going UpDown Stairs\n",
    "6. Walking and Talking with Someone\n",
    "7. Talking while Standing\n",
    "\n",
    "### 5.2 - Load in the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Working Directory:\n",
    "currentDirectory = os.getcwd()\n",
    "\n",
    "# Path to Dataset:\n",
    "path_dataset = currentDirectory + '/Dataset/Activity Recognition_Accelerometer Dataset/'\n",
    "\n",
    "# Read in the dataset:\n",
    "accelerometer_dataset = pd.read_csv(path_dataset + 'Accelerometer Dataset.csv', sep=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1502</td>\n",
       "      <td>2215</td>\n",
       "      <td>2153</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1667</td>\n",
       "      <td>2072</td>\n",
       "      <td>2047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1611</td>\n",
       "      <td>1957</td>\n",
       "      <td>1906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1601</td>\n",
       "      <td>1939</td>\n",
       "      <td>1831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1643</td>\n",
       "      <td>1965</td>\n",
       "      <td>1879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3  4\n",
       "0  0.0  1502  2215  2153  1\n",
       "1  1.0  1667  2072  2047  1\n",
       "2  2.0  1611  1957  1906  1\n",
       "3  3.0  1601  1939  1831  1\n",
       "4  4.0  1643  1965  1879  1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect:\n",
    "accelerometer_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are no column header, so let's add in those now, according to the scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1502</td>\n",
       "      <td>2215</td>\n",
       "      <td>2153</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1667</td>\n",
       "      <td>2072</td>\n",
       "      <td>2047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1611</td>\n",
       "      <td>1957</td>\n",
       "      <td>1906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1601</td>\n",
       "      <td>1939</td>\n",
       "      <td>1831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1643</td>\n",
       "      <td>1965</td>\n",
       "      <td>1879</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     x     y     z  activity\n",
       "0    0.0  1502  2215  2153         1\n",
       "1    1.0  1667  2072  2047         1\n",
       "2    2.0  1611  1957  1906         1\n",
       "3    3.0  1601  1939  1831         1\n",
       "4    4.0  1643  1965  1879         1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the column headers:\n",
    "dat_column_headers = ['index', 'x', 'y', 'z', 'activity']\n",
    "\n",
    "# Set it up:\n",
    "accelerometer_dataset.columns = dat_column_headers\n",
    "\n",
    "# Inspect:\n",
    "accelerometer_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the target variable here is the \"activity\" column. This is the \"y\" variable that the model should predict.\n",
    "\n",
    "### 5.3 - Determine the Null Accuracy of the ML Model:\n",
    "\n",
    "To do this, invoke the \"value_counts()\" method with the \"normalize\" parameter set to true. The largest output value here will be the Null Accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    0.515369\n",
       "1    0.207242\n",
       "4    0.165291\n",
       "3    0.068793\n",
       "5    0.019637\n",
       "6    0.017951\n",
       "2    0.005711\n",
       "0    0.000006\n",
       "Name: activity, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerometer_dataset['activity'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Null accuracy value is 51.53%.\n",
    "\n",
    "### 5.4 - Determine the Base ML model Accuracy:\n",
    "\n",
    "The accuracy result from the model here will be used as the benchmark moving forward. The model to be used here is the: KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Libraries:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 - Split the dataset into Training and Testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Training Set:\n",
    "X = accelerometer_dataset[['x', 'y', 'z']]\n",
    "\n",
    "# Define the Testing Set:\n",
    "y = accelerometer_dataset['activity']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 - Sett up the Grid Search Parameters for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Parameters to Grid Search:\n",
    "knn_params = {'n_neighbors': [3, 4, 5, 6, 7, 8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.3 - Run the KNN Model and Evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\Py37Work\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Best Score: 74.81678310489855% and Best Parameters: {'n_neighbors': 7}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model:\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Grid Search:\n",
    "grid = GridSearchCV(estimator=knn, param_grid=knn_params, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the dataset:\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Print the results:\n",
    "print(\"Model's Best Score: {}% and Best Parameters: {}\".format( (100 * grid.best_score_) , grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score to beat has been determined to be 74.8%.\n",
    "\n",
    "## 6 - Polynomial Features:\n",
    "\n",
    "An important key point to working with numerical data and the creation of more features from it, would be to utilised the \"PolynomialFeatures\" class from scikit-learn. It essentially is a constructor thatt creates new columns that are products of existing columns in the dataset, it captures the feature interactions.\n",
    "\n",
    "Specifically, it will generate a new feature matrix by utilising all of the polynomial combinations of the features with a degress less than or equal to a specified degree. This means that for the example, if the input sample is 2-Dimensional, like [a, b], then the degree-2 polynomial features will be [1, a, b, a^2, ab, b^2]\n",
    "\n",
    "The \"PolynomialFeatures\" class have 3 parameters:\n",
    "1. degree, is the degree of the polynomial features (default = 2)\n",
    "2. interaction_only, is a Boolean, if True outputs only the interaction features. This means the output feattures are products of degree distinct features. (default = False)\n",
    "3. include_bias, is a Boolean, if True will include a Bias column. This means the feature where all polynomial powers are zero, adding a column of all ones. \n",
    "\n",
    "### 6.1 - Set up the Polynomial Feature instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required library:\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162501, 9)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate:\n",
    "poly = PolynomialFeatures(degree=2,\n",
    "                          interaction_only=False,\n",
    "                          include_bias=False)\n",
    "\n",
    "# Fit and Transform on the dataset:\n",
    "X_polynomial = poly.fit_transform(X)\n",
    "\n",
    "# Inspect:\n",
    "X_polynomial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x0^2</th>\n",
       "      <th>x0 x1</th>\n",
       "      <th>x0 x2</th>\n",
       "      <th>x1^2</th>\n",
       "      <th>x1 x2</th>\n",
       "      <th>x2^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1502.0</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>2153.0</td>\n",
       "      <td>2256004.0</td>\n",
       "      <td>3326930.0</td>\n",
       "      <td>3233806.0</td>\n",
       "      <td>4906225.0</td>\n",
       "      <td>4768895.0</td>\n",
       "      <td>4635409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1667.0</td>\n",
       "      <td>2072.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>2778889.0</td>\n",
       "      <td>3454024.0</td>\n",
       "      <td>3412349.0</td>\n",
       "      <td>4293184.0</td>\n",
       "      <td>4241384.0</td>\n",
       "      <td>4190209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1611.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>2595321.0</td>\n",
       "      <td>3152727.0</td>\n",
       "      <td>3070566.0</td>\n",
       "      <td>3829849.0</td>\n",
       "      <td>3730042.0</td>\n",
       "      <td>3632836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1601.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1831.0</td>\n",
       "      <td>2563201.0</td>\n",
       "      <td>3104339.0</td>\n",
       "      <td>2931431.0</td>\n",
       "      <td>3759721.0</td>\n",
       "      <td>3550309.0</td>\n",
       "      <td>3352561.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1643.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>2699449.0</td>\n",
       "      <td>3228495.0</td>\n",
       "      <td>3087197.0</td>\n",
       "      <td>3861225.0</td>\n",
       "      <td>3692235.0</td>\n",
       "      <td>3530641.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x0      x1      x2       x0^2      x0 x1      x0 x2       x1^2  \\\n",
       "0  1502.0  2215.0  2153.0  2256004.0  3326930.0  3233806.0  4906225.0   \n",
       "1  1667.0  2072.0  2047.0  2778889.0  3454024.0  3412349.0  4293184.0   \n",
       "2  1611.0  1957.0  1906.0  2595321.0  3152727.0  3070566.0  3829849.0   \n",
       "3  1601.0  1939.0  1831.0  2563201.0  3104339.0  2931431.0  3759721.0   \n",
       "4  1643.0  1965.0  1879.0  2699449.0  3228495.0  3087197.0  3861225.0   \n",
       "\n",
       "       x1 x2       x2^2  \n",
       "0  4768895.0  4635409.0  \n",
       "1  4241384.0  4190209.0  \n",
       "2  3730042.0  3632836.0  \n",
       "3  3550309.0  3352561.0  \n",
       "4  3692235.0  3530641.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame and Inspect:\n",
    "X_polynomial_df = pd.DataFrame(data=X_polynomial, columns=poly.get_feature_names())\n",
    "X_polynomial_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Exploratory Data Analysis (EDA):\n",
    "\n",
    "With the polynomial features established, this section will go into visualising the dataset to better guage the interactions of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Library:\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27466a97588>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(data=X_polynomial_df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above heatmap, the colour that is lighter means the greater the correlation between these features.\n",
    "\n",
    "### 7.2 - Parameter change: set interaction_only to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162501, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate:\n",
    "poly = PolynomialFeatures(degree=2,\n",
    "                          interaction_only=True,\n",
    "                          include_bias=False)\n",
    "\n",
    "# Fit and Transform on the dataset:\n",
    "X_polynomial = poly.fit_transform(X)\n",
    "\n",
    "# Inspect:\n",
    "X_polynomial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x0 x1</th>\n",
       "      <th>x0 x2</th>\n",
       "      <th>x1 x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1502.0</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>2153.0</td>\n",
       "      <td>3326930.0</td>\n",
       "      <td>3233806.0</td>\n",
       "      <td>4768895.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1667.0</td>\n",
       "      <td>2072.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>3454024.0</td>\n",
       "      <td>3412349.0</td>\n",
       "      <td>4241384.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1611.0</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>3152727.0</td>\n",
       "      <td>3070566.0</td>\n",
       "      <td>3730042.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1601.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1831.0</td>\n",
       "      <td>3104339.0</td>\n",
       "      <td>2931431.0</td>\n",
       "      <td>3550309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1643.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1879.0</td>\n",
       "      <td>3228495.0</td>\n",
       "      <td>3087197.0</td>\n",
       "      <td>3692235.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x0      x1      x2      x0 x1      x0 x2      x1 x2\n",
       "0  1502.0  2215.0  2153.0  3326930.0  3233806.0  4768895.0\n",
       "1  1667.0  2072.0  2047.0  3454024.0  3412349.0  4241384.0\n",
       "2  1611.0  1957.0  1906.0  3152727.0  3070566.0  3730042.0\n",
       "3  1601.0  1939.0  1831.0  3104339.0  2931431.0  3550309.0\n",
       "4  1643.0  1965.0  1879.0  3228495.0  3087197.0  3692235.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame and Inspect:\n",
    "X_polynomial_df = pd.DataFrame(data=X_polynomial, columns=poly.get_feature_names())\n",
    "X_polynomial_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x27466ec2388>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Heatmap:\n",
    "sns.heatmap(data=X_polynomial_df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of columns have been reduced from 9 to 6, which subsequently produced the heatmap above. These are the features that interact with each other.\n",
    "\n",
    "### 7.3 - Run the KNN model with these new features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Grid Search Parameters for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Parameters to Grid Search:\n",
    "pipe_params = {'poly_features__degree': [1, 2, 3, 4, 5],\n",
    "               'poly_features__interaction_only': [True, False],\n",
    "               'classify__n_neighbors': [3, 4, 5, 6, 7, 8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('poly_features', poly), ('classify', knn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the KNN Model and Evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\Py37Work\\lib\\site-packages\\sklearn\\model_selection\\_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Best Score: 74.82% and Best Parameters: {'classify__n_neighbors': 7, 'poly_features__degree': 2, 'poly_features__interaction_only': False}\n"
     ]
    }
   ],
   "source": [
    "# Grid Search:\n",
    "grid = GridSearchCV(estimator=pipe, param_grid=pipe_params, n_jobs=-1)\n",
    "# grid = GridSearchCV(pipe, pipe_params, n_jobs=-1)\n",
    "# Fit the model to the dataset:\n",
    "grid.fit(X, y)\n",
    "\n",
    "# Print the results:\n",
    "print(\"Model's Best Score: {}% and Best Parameters: {}\".format( round((100 * grid.best_score_), ndigits=2) , grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmarked accuracy before was 74.8%. The model with polynomial features was able to achieve 74.82%, this can be seen to be only a slight improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Text-specific Feature Construction:\n",
    "\n",
    "From this point on, the categorical data will be in the form of __Strings__. This kind of data is much more complex, as the dataset will be a series of categories (a.k.a tokens).\n",
    "\n",
    "This type of work would be called __Natural Language Processing (NLP)__. As mentioned before, all ML models will require the input to be in numerical values, therefore there are techniques to convert text data into numerical features.\n",
    "\n",
    "### 8.1 - Dataset:\n",
    "\n",
    "Link: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/\n",
    "\n",
    "Dataset Description:\n",
    "\n",
    "The Twitter Sentiment Analysis Dataset contains 1,578,627 classified tweets, each row is marked as 1 for positive sentiment and 0 for negative sentiment.\n",
    "\n",
    "### 8.2 - Bag of Words Representation:\n",
    "\n",
    "Scikit-learn does have a module that is the \"feature_extraction\" to extract features from text data. From here on out, the text data will be refered to as \"Corpus\", which means the aggregate of text content or documents.\n",
    "\n",
    "A vectorisation technique called \"__Bag-og-words__\" allows for the transformation of a corpus into numerical representations. It finds the occuences of words while ignoring the positioning of the words in the document. \n",
    "\n",
    "A Bag-of-words is achieved with 3 steps:\n",
    "1. Tokenising, it uses white spaces and punctuation to separate the words from each other and turns them in tokens, where each possible token has an iteger ID.\n",
    "2. Counting, countts the occurrences of the tokens within the document.\n",
    "3. Normalising, the tokens are then weighted with diminishing importance when they occur in the majority of the documents.\n",
    "\n",
    "There are also 2 more:\n",
    "1. CountVectorizer.\n",
    "2. TF-idf vectorizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 - CountVectorizer:\n",
    "\n",
    "This is one of the most commonly used method to convert text data into vector representations. It converts the text columns into matrices, whereby the columns are ttoken and the cell values are counts of occurrences. The resultant matrix is called \"document-term matrix\".\n",
    "\n",
    "### 8.3.1 - CountVectorizer Parameters:\n",
    "\n",
    "The parameters are:\n",
    "- stop_words, set to \"English\" to use the built-in list of stop words. Removes stop words (Unwanted Noise) from the tokens.\n",
    "- min_df, to skim the number of features by ignoring less frequent terms of a given threshold.\n",
    "- max_df, to include only words that occur the most, set by a given threshold.\n",
    "- ngram_range, takes in a tuple of [lower, upper] boundary of the range of n-values, this indicates the number of different n-grams to be extracted. N-grams represent Phrases.\n",
    "- analyzer, determines if the feautre should be made of word or character n-grams.\n",
    "\n",
    "## 8.4 - Load in the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Working Directory:\n",
    "currentDirectory = os.getcwd()\n",
    "\n",
    "# Path to Dataset:\n",
    "path_dataset = currentDirectory + '/Dataset/Tweets Dataset/'\n",
    "\n",
    "# Read in the dataset:\n",
    "tweets_data_df = pd.read_csv(path_dataset + 'twitter_sentiment.csv', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect:\n",
    "tweets_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first column: ItemID\n",
    "del tweets_data_df['ItemID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 - Perform the Feature Extraction: with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required library:\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into X and Y: \n",
    "\n",
    "X for the training data and Y for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split:\n",
    "X_tweets = tweets_data_df['SentimentText']\n",
    "\n",
    "y_tweets = tweets_data_df['Sentiment']\n",
    "\n",
    "# Inpect the shape:\n",
    "X_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer:\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now 105,849 columns, whereas there was none in the original dataset.\n",
    "\n",
    "### 8.5.1 - Stop Words:\n",
    "\n",
    "Stop words are the Noise in a text dataset, where they are commonly used words in the specified language, like if, a, the and so on. (https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105545)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are now 105,545 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.2 - min_df:\n",
    "\n",
    "This will only include (min_df=0.05) words that occur in at least 5% of the corpus documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 31)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with min_df\n",
    "vect = CountVectorizer(min_df=0.05)\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.3 - max_df:\n",
    "\n",
    "This will only include (max_df=0.8) words that occur in at most 80% of the corpus documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with max_df\n",
    "vect = CountVectorizer(max_df= 0.80)\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.4 - ngram_range:\n",
    "\n",
    "Inclues phrases up to 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 3219557)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with  ngram_range\n",
    "vect = CountVectorizer(ngram_range= (1, 5))\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.5 -  Have a look at the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 01',\n",
       " '00 01 minute',\n",
       " '00 01 minute after',\n",
       " '00 01 minute after applications',\n",
       " '00 10',\n",
       " '00 10 00',\n",
       " '00 56',\n",
       " '00 56 03',\n",
       " '00 56 03 beating',\n",
       " '00 56 03 beating my',\n",
       " '00 always',\n",
       " '00 always have',\n",
       " '00 always have such',\n",
       " '00 always have such warmth']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.6 - lowercase:\n",
    "\n",
    "Changes all the text data to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with lowercase\n",
    "vect = CountVectorizer(lowercase=True)\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.7 - max_features:\n",
    "\n",
    "Places hard limits on the features based on max counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with max_features\n",
    "vect = CountVectorizer(max_features=1000)\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5.8 - analyzer:\n",
    "\n",
    "For analyzer='word': By using the default (word) analyzer, it will split the corpus into words.\n",
    "For analyzer='char': Uses the characters.\n",
    "For analyzer='char_wb': Here, it uses characters again but only those are aren't at the beginning or ends of words wb stands for word boudnaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with analyzer\n",
    "vect = CountVectorizer(analyzer='word')\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0001t',\n",
       " '000martha',\n",
       " '001',\n",
       " '0010x0010',\n",
       " '0022',\n",
       " '007',\n",
       " '007heather007',\n",
       " '007hil']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 153)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with analyzer\n",
    "vect = CountVectorizer(analyzer='char')\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 149)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with analyzer\n",
    "vect = CountVectorizer(analyzer='char_wb')\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 - Building a Custom Analyzer:\n",
    "\n",
    "From a concept perspective, words are built from root words (a.k.a stems) and from these, a custom analyzer can be built.\n",
    "\n",
    "__Stemming__ is a common NLP method that allows for the stemming of vocabulary, meaning it is made smaller by converting the words into its roots. A great toolkit to do this is __NLTK__ package, and another is the __Stemmer__ package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries:\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate:\n",
    "stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('interest', True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of stemming:\n",
    "stemmer.stem('interesting'), stemmer.stem('interesting') == stemmer.stem('interest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6.1 - Create a function that allows for tokenising words into their stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns a list of lemmas:\n",
    "def word_tokenise(text, how='lemma'):\n",
    "    \n",
    "    # tokenise into words:\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'you', 'are', 'veri', 'interest']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it:\n",
    "word_tokenise(\"hello you are very interesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 154397)\n"
     ]
    }
   ],
   "source": [
    "# Apply it as a custom analyser: custom\n",
    "# Instantiate the CountVectorizer: with analyzer\n",
    "vect = CountVectorizer(analyzer=word_tokenise)\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, CountVectorizer is useful in expanding the features and at the same time, converts the text into numerical features.\n",
    "\n",
    "## 8.7 - Tf-idf Vectorizer: Term Frequency-Inverse Document Frequency\n",
    "\n",
    "Tfidf stands for \"Term FrequencyInverse Document Frequency\" is a numerical statistic used to reflect how important a word is to a document in a collection or corpus of documents. TFIDF is used as a weighting factor during text search processes and text mining.\n",
    "\n",
    "- Intuition: if a word appears several times in a given document, this word might be meaningful (more important) than other words that appeared fewer times in the same document. However, if a given word appeared several times in a given document but also appeared many times in other documents, there is a probability that this word might be common frequent word such as 'I' 'am'..etc. (not really important or meaningful!).\n",
    "\n",
    "- TF: Term Frequency is used to measure the frequency of term occurrence in a document: TF(word) = Number of times the 'word' appears in a document / Total number of terms in the document\n",
    "\n",
    "- IDF: Inverse Document Frequency is used to measure how important a term is: IDF(word) = log_e(Total number of documents / Number of documents with the term 'word' in it).\n",
    "\n",
    "__For Example__: \n",
    "\n",
    "Let's assume we have a document that contains 1000 words and the term John appeared 20 times, the Term-Frequency for the word 'John' can be calculated as follows: -> TF | john = 20/1000 = 0.02\n",
    "\n",
    "Let's calculate the IDF (inverse document frequency) of the word 'john' assuming that it appears 50,000 times in a 1,000,000 million documents (corpus). -> IDF | john = log (1,000,000/50,000) = 1.3\n",
    "\n",
    "Therefore the overall weight of the word 'john' is as follows -> TF-IDF | john = 0.02 * 1.3 = 0.026\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Libraries:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Default CountVectorizer():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849) 6.613194267305311e-05\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the CountVectorizer: with defaults\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape, _[0, :].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With TfidfVectorizer():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 105849) 2.1863060975751192e-05\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the TfidfVectorizer: with defaults\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform on the dataset with CountVectorizer:\n",
    "_ = vect .fit_transform(X_tweets)\n",
    "\n",
    "# Inspect the Shape of the output:\n",
    "print(_.shape, _[0, :].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, both vectorisers do output the same number of rows and columns, however, the values in each cell is different. This is because, the way in which the cells are filled is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Machine Learning Pipelines with Text:\n",
    "\n",
    "Recall that the goal is to convert text data into numerical values that would be acceptable by ML models. Both thhe CountVectorizer and TfidVectorizer are \"transformers\", this means that it will work well with scikit-learn's pipeline. \n",
    "\n",
    "The classifier employed here will be the __Naive Bayes__ Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Libraries:\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 - Define the NULL accuracry: \n",
    "\n",
    "For the purposes of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.564632\n",
       "0    0.435368\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null Accuracy:\n",
    "y_tweets.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, the null accuracy here is 56.46%.\n",
    "\n",
    "### 9.2 - Build the Pipeline:\n",
    "\n",
    "The sequence for this pipeline will be:\n",
    "1. CountVectorizer() to be used to featureise the tweets (text data).\n",
    "2. MultiNomialNB(), is the Naive Bayes model that will perform the classification task to output a positive or negative sentiment (sentiment anaylsis classification task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definte the Pipeline Parameters:\n",
    "pipe_params = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'vect__max_features': [1000, 10000],\n",
    "               'vect__stop_words': [None, 'english']\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Best Score: 75.59% and Best Parameters: {'vect__max_features': 10000, 'vect__ngram_range': (1, 2), 'vect__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Pipeline:\n",
    "pipe_tweets = Pipeline(steps=[('vect', CountVectorizer()),\n",
    "                              ('classify', MultinomialNB())\n",
    "                             ]\n",
    "                      )\n",
    "\n",
    "# Instantiate the GridSearvhCV object:\n",
    "grid = GridSearchCV(estimator=pipe_tweets, param_grid=pipe_params, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the dataset:\n",
    "grid.fit(X_tweets, y_tweets)\n",
    "\n",
    "# Print the results:\n",
    "print(\"Model's Best Score: {}% and Best Parameters: {}\".format( round((100 * grid.best_score_), ndigits=2) , grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 - Horizontal Stacking of Features:\n",
    "\n",
    "Rather than rebuilding the pipeline with tfidVectorizer(), this section will demonstrate the use of the \"FeatureUnion\" module from sklearn. It supports the horizontal stacking of features, meaning the features are placed side by side. More importantly, it allows the use of multiple types of text featurisers within the same pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries:\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 211698)\n"
     ]
    }
   ],
   "source": [
    "# Build the Featuriser Object:\n",
    "featuriser = FeatureUnion(transformer_list=[('tfidf_vect', TfidfVectorizer()),\n",
    "                                            ('count_vect', CountVectorizer())],\n",
    "                          n_jobs=-1\n",
    "                         )\n",
    "\n",
    "# Fit and transform on the dataset:\n",
    "_ = featuriser.fit_transform(X_tweets)\n",
    "\n",
    "# Inspect:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows twice as many columns, as a result of the \"side-by-side\" data in the cells. This allows the ML model to learn from __BOTH__ sets of data at the same time.\n",
    "\n",
    "### 9.3.1 - Change the parameters of the Featuriser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 400)\n"
     ]
    }
   ],
   "source": [
    "# Change the parameters:\n",
    "featuriser.set_params(tfidf_vect__max_features=100, \n",
    "                      count_vect__ngram_range=(1, 2), \n",
    "                      count_vect__max_features=300)\n",
    "\n",
    "# Fit and transform on the dataset:\n",
    "_ = featuriser.fit_transform(X_tweets)\n",
    "\n",
    "# Inspect:\n",
    "print(_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 - Build a more Comprehensive Pipeline and Classify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running time calculation\n",
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Best Score: 75.81% and Best Parameters: {'featuriser__count_vect__max_features': 10000, 'featuriser__count_vect__ngram_range': (1, 2), 'featuriser__count_vect__stop_words': None, 'featuriser__tfidf_vect__max_features': 10000, 'featuriser__tfidf_vect__ngram_range': (1, 1), 'featuriser__tfidf_vect__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "# Define the Pipeline Parameters:\n",
    "pipe_params = {'featuriser__count_vect__ngram_range':[(1, 1), (1, 2)], \n",
    "               'featuriser__count_vect__max_features':[1000, 10000], \n",
    "               'featuriser__count_vect__stop_words':[None, 'english'],\n",
    "              'featuriser__tfidf_vect__ngram_range':[(1, 1), (1, 2)], \n",
    "               'featuriser__tfidf_vect__max_features':[1000, 10000], \n",
    "               'featuriser__tfidf_vect__stop_words':[None, 'english']}\n",
    "\n",
    "# Instantiate the Pipeline:\n",
    "pipe_tweets = Pipeline(steps=[('featuriser', featuriser),\n",
    "                              ('classify', MultinomialNB())\n",
    "                             ]\n",
    "                      )\n",
    "\n",
    "# Instantiate the GridSearvhCV object:\n",
    "grid = GridSearchCV(estimator=pipe_tweets, param_grid=pipe_params, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the dataset:\n",
    "grid.fit(X_tweets, y_tweets)\n",
    "\n",
    "# Print the results:\n",
    "print(\"Model's Best Score: {}% and Best Parameters: {}\".format( round((100 * grid.best_score_), ndigits=2) , grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.78 Minutes\n",
      "Time: 0.05 hours\n"
     ]
    }
   ],
   "source": [
    "# Stop the timer:\n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} Minutes'.format(round((stop - start)/60, 2)))\n",
    "print('Time: {} hours'.format(round((stop - start)/3600, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "The model only increase slightly in its accuracy (by 0.20%) from learning with bothh datasets.\n",
    "\n",
    "## Summary:\n",
    "\n",
    "This project has allowed me to learn a great deal about implementing classes that deals with features in the form of categorical, numerical and text into ML model compatible numerical values. These classes are useful as it is compatible with the sklearn library and its models, where I also learn how to implement them in a Pipeline where it helps to streamline the entire process. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
